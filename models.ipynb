{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf062ba-e448-4b45-9733-5bf77f97f10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389a058-45bc-46a4-ba5b-85675ce70e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a2203-efcd-4fd4-a508-ddb407a1b2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0e1096",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     17\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m1234\u001b[39m)\n\u001b[0;32m     18\u001b[0m epochs_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# number of epochs for the neural networks\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, confusion_matrix, \\\n",
    "    precision_recall_fscore_support, roc_auc_score\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, Flatten, Conv2D\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "epochs_number = 1  # number of epochs for the neural networks\n",
    "test_set_size = 0.1  # percentage of the test size comparing to the whole dataset\n",
    "oversampling_flag = 0  # set to 1 to over-sample the minority class\n",
    "oversampling_percentage = 0.2  # percentage of the minority class after the oversampling comparing to majority class\n",
    "\n",
    "\n",
    "# Definition of functions\n",
    "def read_data():\n",
    "    rawData = pd.read_csv('preprocessedR.csv')\n",
    "\n",
    "    # Setting the target and dropping the unnecessary columns\n",
    "    y = rawData[['FLAG']]\n",
    "    X = rawData.drop(['FLAG', 'CONS_NO'], axis=1)\n",
    "\n",
    "    print('Normal Consumers:                    ', y[y['FLAG'] == 0].count()[0])\n",
    "    print('Consumers with Fraud:                ', y[y['FLAG'] == 1].count()[0])\n",
    "    print('Total Consumers:                     ', y.shape[0])\n",
    "    print(\"Classification assuming no fraud:     %.2f\" % (y[y['FLAG'] == 0].count()[0] / y.shape[0] * 100), \"%\")\n",
    "\n",
    "    # columns reindexing according to dates\n",
    "    X.columns = pd.to_datetime(X.columns)\n",
    "    X = X.reindex(X.columns, axis=1)\n",
    "\n",
    "    # Splitting the dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y['FLAG'], test_size=test_set_size, random_state=0)\n",
    "    print(\"Test set assuming no fraud:           %.2f\" % (y_test[y_test == 0].count() / y_test.shape[0] * 100), \"%\\n\")\n",
    "\n",
    "    # Oversampling of minority class to encounter the imbalanced learning\n",
    "    if oversampling_flag == 1:\n",
    "        over = SMOTE(sampling_strategy=oversampling_percentage, random_state=0)\n",
    "        X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "        print(\"Oversampling statistics in training set: \")\n",
    "        print('Normal Consumers:                    ', y_train[y_train == 0].count())\n",
    "        print('Consumers with Fraud:                ', y_train[y_train == 1].count())\n",
    "        print(\"Total Consumers                      \", X_train.shape[0])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def results(y_test, prediction):\n",
    "    print(\"Accuracy\", 100 * accuracy_score(y_test, prediction))\n",
    "    print(\"RMSE:\", mean_squared_error(y_test, prediction, squared=False))\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, prediction))\n",
    "    print(\"F1:\", 100 * precision_recall_fscore_support(y_test, prediction)[2])\n",
    "    print(\"AUC:\", 100 * roc_auc_score(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction), \"\\n\")\n",
    "\n",
    "\n",
    "def ANN(X_train, X_test, y_train, y_test):\n",
    "    print('Artificial Neural Network:')\n",
    "    # for i in range(4,100,3):\n",
    "    #     print(\"Epoch:\",i)\n",
    "\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=1034, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # model.fit(X_train, y_train, validation_split=0, epochs=i, shuffle=True, verbose=0)\n",
    "    model.fit(X_train, y_train, validation_split=0, epochs=epochs_number, shuffle=True, verbose=1)\n",
    "    prediction = model.predict_classes(X_test)\n",
    "    model.summary()\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def CNN1D(X_train, X_test, y_train, y_test):\n",
    "    print('1D - Convolutional Neural Network:')\n",
    "\n",
    "    # Transforming the dataset into tensors\n",
    "    X_train = X_train.to_numpy().reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.to_numpy().reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(100, kernel_size=7, input_shape=(1034, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # model.fit(X_train, y_train, epochs=1, validation_split=0.1, shuffle=False, verbose=1)\n",
    "    model.fit(X_train, y_train, epochs=epochs_number, validation_split=0, shuffle=False, verbose=1)\n",
    "    prediction = model.predict_classes(X_test)\n",
    "    model.summary()\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def CNN2D(X_train, X_test, y_train, y_test):\n",
    "    print('2D - Convolutional Neural Network:')\n",
    "\n",
    "    # Transforming every row of the train set into a 2D array and then into a tensor\n",
    "    n_array_X_train = X_train.to_numpy()\n",
    "    n_array_X_train_extended = np.hstack((n_array_X_train, np.zeros(\n",
    "        (n_array_X_train.shape[0], 2))))  # adding two empty columns in order to make the number of columns\n",
    "    # an exact multiple of 7\n",
    "    week = []\n",
    "    for i in range(n_array_X_train_extended.shape[0]):\n",
    "        a = np.reshape(n_array_X_train_extended[i], (-1, 7, 1))\n",
    "        week.append(a)\n",
    "    X_train_reshaped = np.array(week)\n",
    "\n",
    "    # Transforming every row of the train set into a 2D array and then into a tensor\n",
    "    n_array_X_test = X_test.to_numpy()  # X_test to 2D - array\n",
    "    n_array_X_train_extended = np.hstack((n_array_X_test, np.zeros((n_array_X_test.shape[0], 2))))\n",
    "    week2 = []\n",
    "    for i in range(n_array_X_train_extended.shape[0]):\n",
    "        b = np.reshape(n_array_X_train_extended[i], (-1, 7, 1))\n",
    "        week2.append(b)\n",
    "    X_test_reshaped = np.array(week2)\n",
    "\n",
    "    input_shape = (1, 148, 7, 1)  # input shape of the tensor\n",
    "\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(kernel_size=(7, 3), filters=32, input_shape=input_shape[1:], activation='relu',\n",
    "                     data_format='channels_last'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    # model.summary()\n",
    "    #     model.fit(X_train_reshaped, y_train, validation_split=0.1, epochs=i, shuffle=False, verbose=0)\n",
    "    model.fit(X_train_reshaped, y_train, validation_split=0.1, epochs=epochs_number, shuffle=False, verbose=1)\n",
    "\n",
    "    # prediction = model.predict_classes(X_test)\n",
    "    prediction = model.predict_classes(X_test_reshaped)\n",
    "    model.summary()\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def LR(X_train, X_test, y_train, y_test):\n",
    "    print('Logistic Regression:')\n",
    "    '''\n",
    "    # Parameters selection \n",
    "    param_grid = {'C': [0.1,1,10,100],'solver': ['newton-cg', 'lbfgs']}\n",
    "    grid = GridSearchCV(LogisticRegression(max_iter=1000,random_state=0), param_grid=param_grid, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    df = pd.DataFrame(grid.cv_results_)\n",
    "    print(df[['param_C', 'param_solver', 'mean_test_score', 'rank_test_score']])\n",
    "    '''\n",
    "    model = LogisticRegression(C=1000, max_iter=1000, n_jobs=-1, solver='newton-cg')\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def DT(X_train, X_test, y_train, y_test):\n",
    "    print('Decision Tree:')\n",
    "    model = DecisionTreeClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def RF(X_train, X_test, y_train, y_test):\n",
    "    print('Random Forest:')\n",
    "    '''\n",
    "    # Parameters selection \n",
    "    param_grid = {'n_estimators':[10,100,1000]}\n",
    "    grid = GridSearchCV(RandomForestClassifier(random_state=0), param_grid=param_grid, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    df = pd.DataFrame(grid.cv_results_)\n",
    "    print(df[['param_criterion', 'mean_test_score', 'rank_test_score']])\n",
    "    '''\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, min_samples_leaf=1, max_features='auto',  # max_depth=10,\n",
    "                                   random_state=0, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def SVM(X_train, X_test, y_train, y_test):\n",
    "    model = SVC(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "# ----Main----\n",
    "X_train, X_test, y_train, y_test = read_data()\n",
    "\n",
    "# Uncomment any model to test\n",
    "# ANN(X_train, X_test, y_train, y_test)\n",
    "# CNN1D(X_train, X_test, y_train, y_test)\n",
    "# CNN2D(X_train, X_test, y_train, y_test)\n",
    "# RF(X_train, X_test, y_train, y_test)\n",
    "# LR(X_train, X_test, y_train, y_test)\n",
    "# DT(X_train, X_test, y_train, y_test)\n",
    "# SVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fba21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\python\\envs\\tf\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\envs\\tf\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB 8.9 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/11.6 MB 9.8 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/11.6 MB 7.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.1/11.6 MB 6.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 4.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/11.6 MB 5.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.0/11.6 MB 4.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.2/11.6 MB 4.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.4/11.6 MB 4.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/11.6 MB 4.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.7/11.6 MB 4.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/11.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.3/11.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.5/11.6 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.6/11.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.8/11.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.0/11.6 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.2/11.6 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.4/11.6 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.9/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.1/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.3/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.7/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.2/11.6 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.6 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.6 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.7/11.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.1/11.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.3/11.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.5/11.6 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.7/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.8/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.0/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.2/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.4/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.0/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.1/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.3/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.5/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.7/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.9/11.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.4/11.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.6/11.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.8/11.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.9/11.6 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.6 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
      "   --------------------------------------  337.9/345.4 kB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 345.4/345.4 kB 7.1 MB/s eta 0:00:00\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5ac328-88d4-42c0-bb20-86bfdc746cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\python\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\python\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.2-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/10.6 MB 6.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/10.6 MB 7.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/10.6 MB 6.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/10.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/10.6 MB 5.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.4/10.6 MB 5.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/10.6 MB 4.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/10.6 MB 4.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.0/10.6 MB 4.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.2/10.6 MB 4.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.4/10.6 MB 4.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.5/10.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.7/10.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/10.6 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.1/10.6 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.3/10.6 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.5/10.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.6/10.6 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.8/10.6 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.0/10.6 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.2/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.4/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.7/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.8/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.0/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.2/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.3/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.6/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.7/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.9/10.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.1/10.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.5/10.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.7/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.8/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.0/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.2/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.4/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.7/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.1/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.3/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.5/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.8/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.0/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.2/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.6/10.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.7/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.9/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.1/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.3/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.4.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28aadc02-d332-41b7-b6cf-a15cff4a076a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\python\\envs\\tf\\lib\\site-packages (1.26.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7fdc68-df7d-42ae-95e6-3eb71299ccac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.12.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\python\\envs\\tf\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\python\\envs\\tf\\lib\\site-packages (from imbalanced-learn->imblearn) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in d:\\python\\envs\\tf\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\python\\envs\\tf\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\envs\\tf\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.12.2-py3-none-any.whl (257 kB)\n",
      "   ---------------------------------------- 0.0/258.0 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/258.0 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 258.0/258.0 kB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.12.2 imblearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9617e9af-0807-4c4f-81af-30a0389cf37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Consumers:                     36677\n",
      "Consumers with Fraud:                 3579\n",
      "Total Consumers:                      40256\n",
      "Classification assuming no fraud:     91.11 %\n",
      "Test set assuming no fraud:           90.78 %\n",
      "\n",
      "2D - Convolutional Neural Network:\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.2737 - accuracy: 0.9120 - val_loss: 0.2523 - val_accuracy: 0.9144\n",
      "126/126 [==============================] - 0s 2ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " module_wrapper (ModuleWrap  (None, 142, 5, 32)        704       \n",
      " per)                                                            \n",
      "                                                                 \n",
      " module_wrapper_1 (ModuleWr  (None, 22720)             0         \n",
      " apper)                                                          \n",
      "                                                                 \n",
      " module_wrapper_2 (ModuleWr  (None, 100)               2272100   \n",
      " apper)                                                          \n",
      "                                                                 \n",
      " module_wrapper_3 (ModuleWr  (None, 100)               10100     \n",
      " apper)                                                          \n",
      "                                                                 \n",
      " module_wrapper_4 (ModuleWr  (None, 64)                6464      \n",
      " apper)                                                          \n",
      "                                                                 \n",
      " module_wrapper_5 (ModuleWr  (None, 1)                 65        \n",
      " apper)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2289433 (8.73 MB)\n",
      "Trainable params: 2289433 (8.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Accuracy 91.15747640337804\n",
      "RMSE: 0.2973638107877614\n",
      "MAE: 0.08842523596621957\n",
      "F1: [95.34031414 13.59223301]\n",
      "AUC: 53.595746328368996\n",
      "[[3642   13]\n",
      " [ 343   28]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, confusion_matrix, \\\n",
    "    precision_recall_fscore_support, roc_auc_score\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, Flatten, Conv2D\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "epochs_number = 1  # number of epochs for the neural networks\n",
    "test_set_size = 0.1  # percentage of the test size comparing to the whole dataset\n",
    "oversampling_flag = 0  # set to 1 to over-sample the minority class\n",
    "oversampling_percentage = 0.2  # percentage of the minority class after the oversampling comparing to majority class\n",
    "\n",
    "\n",
    "# Definition of functions\n",
    "def read_data():\n",
    "    rawData = pd.read_csv('preprocessedR.csv')\n",
    "\n",
    "    # Setting the target and dropping the unnecessary columns\n",
    "    y = rawData[['FLAG']]\n",
    "    X = rawData.drop(['FLAG', 'CONS_NO'], axis=1)\n",
    "\n",
    "    print('Normal Consumers:                    ', y[y['FLAG'] == 0].count()[0])\n",
    "    print('Consumers with Fraud:                ', y[y['FLAG'] == 1].count()[0])\n",
    "    print('Total Consumers:                     ', y.shape[0])\n",
    "    print(\"Classification assuming no fraud:     %.2f\" % (y[y['FLAG'] == 0].count()[0] / y.shape[0] * 100), \"%\")\n",
    "\n",
    "    # columns reindexing according to dates\n",
    "    X.columns = pd.to_datetime(X.columns)\n",
    "    X = X.reindex(X.columns, axis=1)\n",
    "\n",
    "    # Splitting the dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y['FLAG'], test_size=test_set_size, random_state=0)\n",
    "    print(\"Test set assuming no fraud:           %.2f\" % (y_test[y_test == 0].count() / y_test.shape[0] * 100), \"%\\n\")\n",
    "\n",
    "    # Oversampling of minority class to encounter the imbalanced learning\n",
    "    if oversampling_flag == 1:\n",
    "        over = SMOTE(sampling_strategy=oversampling_percentage, random_state=0)\n",
    "        X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "        print(\"Oversampling statistics in training set: \")\n",
    "        print('Normal Consumers:                    ', y_train[y_train == 0].count())\n",
    "        print('Consumers with Fraud:                ', y_train[y_train == 1].count())\n",
    "        print(\"Total Consumers                      \", X_train.shape[0])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def results(y_test, prediction):\n",
    "    print(\"Accuracy\", 100 * accuracy_score(y_test, prediction))\n",
    "    print(\"RMSE:\", mean_squared_error(y_test, prediction, squared=False))\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, prediction))\n",
    "    print(\"F1:\", 100 * precision_recall_fscore_support(y_test, prediction)[2])\n",
    "    print(\"AUC:\", 100 * roc_auc_score(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction), \"\\n\")\n",
    "\n",
    "\n",
    "def ANN(X_train, X_test, y_train, y_test):\n",
    "    print('Artificial Neural Network:')\n",
    "    # for i in range(4,100,3):\n",
    "    #     print(\"Epoch:\",i)\n",
    "\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=1034, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # model.fit(X_train, y_train, validation_split=0, epochs=i, shuffle=True, verbose=0)\n",
    "    model.fit(X_train, y_train, validation_split=0, epochs=epochs_number, shuffle=True, verbose=1)\n",
    "    prediction = (model.predict(X_test)>0.5).astype(\"int32\")\n",
    "    model.summary()\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def CNN1D(X_train, X_test, y_train, y_test):\n",
    "    print('1D - Convolutional Neural Network:')\n",
    "\n",
    "    # Transforming the dataset into tensors\n",
    "    X_train = X_train.to_numpy().reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.to_numpy().reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(100, kernel_size=7, input_shape=(1034, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # model.fit(X_train, y_train, epochs=1, validation_split=0.1, shuffle=False, verbose=1)\n",
    "    model.fit(X_train, y_train, epochs=epochs_number, validation_split=0, shuffle=False, verbose=1)\n",
    "    prediction = (model.predict(X_test)>0.5).astype(\"int32\")\n",
    "    model.summary()\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def CNN2D(X_train, X_test, y_train, y_test):\n",
    "    print('2D - Convolutional Neural Network:')\n",
    "\n",
    "    # Transforming every row of the train set into a 2D array and then into a tensor\n",
    "    n_array_X_train = X_train.to_numpy()\n",
    "    n_array_X_train_extended = np.hstack((n_array_X_train, np.zeros(\n",
    "        (n_array_X_train.shape[0], 2))))  # adding two empty columns in order to make the number of columns\n",
    "    # an exact multiple of 7\n",
    "    week = []\n",
    "    for i in range(n_array_X_train_extended.shape[0]):\n",
    "        a = np.reshape(n_array_X_train_extended[i], (-1, 7, 1))\n",
    "        week.append(a)\n",
    "    X_train_reshaped = np.array(week)\n",
    "\n",
    "    # Transforming every row of the train set into a 2D array and then into a tensor\n",
    "    n_array_X_test = X_test.to_numpy()  # X_test to 2D - array\n",
    "    n_array_X_train_extended = np.hstack((n_array_X_test, np.zeros((n_array_X_test.shape[0], 2))))\n",
    "    week2 = []\n",
    "    for i in range(n_array_X_train_extended.shape[0]):\n",
    "        b = np.reshape(n_array_X_train_extended[i], (-1, 7, 1))\n",
    "        week2.append(b)\n",
    "    X_test_reshaped = np.array(week2)\n",
    "\n",
    "    input_shape = (1, 148, 7, 1)  # input shape of the tensor\n",
    "\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(kernel_size=(7, 3), filters=32, input_shape=input_shape[1:], activation='relu',\n",
    "                     data_format='channels_last'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    # model.summary()\n",
    "    #     model.fit(X_train_reshaped, y_train, validation_split=0.1, epochs=i, shuffle=False, verbose=0)\n",
    "    model.fit(X_train_reshaped, y_train, validation_split=0.1, epochs=epochs_number, shuffle=False, verbose=1)\n",
    "\n",
    "    # prediction = model.predict_classes(X_test)\n",
    "    prediction = (model.predict(X_test_reshaped)>0.5).astype(\"int32\")\n",
    "    model.summary()\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def LR(X_train, X_test, y_train, y_test):\n",
    "    print('Logistic Regression:')\n",
    "    '''\n",
    "    # Parameters selection \n",
    "    param_grid = {'C': [0.1,1,10,100],'solver': ['newton-cg', 'lbfgs']}\n",
    "    grid = GridSearchCV(LogisticRegression(max_iter=1000,random_state=0), param_grid=param_grid, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    df = pd.DataFrame(grid.cv_results_)\n",
    "    print(df[['param_C', 'param_solver', 'mean_test_score', 'rank_test_score']])\n",
    "    '''\n",
    "    model = LogisticRegression(C=1000, max_iter=1000, n_jobs=-1, solver='newton-cg')\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = (model.predict(X_test)>0.5).astype(\"int32\")\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def DT(X_train, X_test, y_train, y_test):\n",
    "    print('Decision Tree:')\n",
    "    model = DecisionTreeClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = (model.predict(X_test)>0.5).astype(\"int32\")\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def RF(X_train, X_test, y_train, y_test):\n",
    "    print('Random Forest:')\n",
    "    '''\n",
    "    # Parameters selection \n",
    "    param_grid = {'n_estimators':[10,100,1000]}\n",
    "    grid = GridSearchCV(RandomForestClassifier(random_state=0), param_grid=param_grid, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    df = pd.DataFrame(grid.cv_results_)\n",
    "    print(df[['param_criterion', 'mean_test_score', 'rank_test_score']])\n",
    "    '''\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, min_samples_leaf=1, max_features='auto',  # max_depth=10,\n",
    "                                   random_state=0, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "def SVM(X_train, X_test, y_train, y_test):\n",
    "    model = SVC(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    results(y_test, prediction)\n",
    "\n",
    "\n",
    "# ----Main----\n",
    "X_train, X_test, y_train, y_test = read_data()\n",
    "\n",
    "# Uncomment any model to test\n",
    "#ANN(X_train, X_test, y_train, y_test)\n",
    "# CNN1D(X_train, X_test, y_train, y_test)\n",
    "CNN2D(X_train, X_test, y_train, y_test)\n",
    "# RF(X_train, X_test, y_train, y_test)\n",
    "# LR(X_train, X_test, y_train, y_test)\n",
    "# DT(X_train, X_test, y_train, y_test)\n",
    "# SVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba4d15-58bf-454a-a265-632c42745031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
